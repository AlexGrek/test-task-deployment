# Concerns

У мене є багато технічних питань.

Що по стейджингу? Чи він буде? Чи він буде там, де є зараз? Чи є інтеграційні тести, які треба ганяти перед деплоєм в прод?
Чи треба їм якісь ресурси?

Чи має деплой в прод буди постійним та автоматичним, чи запускатись вручну, чи при мержі в release чи main бранчі?
Це залежить від структури вашого репозиторія.

Що таке цей workloadengine, що він робить, з ким спілкується, чи треба до нього доступ з інтернету? Чи потрібно йому
мати якийсь scaling/balancing, персістенс, чи варто виділяти під нього окрему ноду? Як він інтегрується з кубером?

Я не писав шаблони для Grafana та іншої дрібноти, але це дуже просто. Лише займає час.

Чи є сенс виділяти певний розмір сховища? Якщо база ніколи не перевищить, скажімо, 8гб (або коли перевищить - ми збільшимо розмір)
то можна створити PV/PVC певного розміру (docker volume типу) для кожної бази.
Інший варіант - монтувати хост-директорію, диск буде обмежений лише віртуалкою, але тоді поди бази даних будуть прив'язані до конкретної ноди.
Варто провести аналіз використання сховища та обрати оптимальну стратегію. Стандарт для проду - обмежені за розміром volume.

Як використовується Redis? У якості кеша? Або інакше? Є сенс розмістити його або на ноді з додатком, або на ноді 
з базами даних для зменшення network latency. Також від цього залежить, скільки виділяти пам'яті під редіс под.

# Workload distribution

Ще одна перевага кубера в тому, що всі ці варіанти можна змінювати прямо на живій системі з мінімальним або нульовим downtime (перезапуск контейнерів)

## Варіант 1

- база на окремій ноді

Даємо ноді роль "сторедж"

```sh
kubectl label node <node-name> <key>=<value>  # приклад
kubectl label node worker-node node-role=storage
```

Прописуємо nodeSelector для нашого поду, щоб він запускався лише там

```yaml
    spec:
      nodeSelector:
        role: storage
```

- workloadengine на окремій ноді

Виділяємо ноду під важкі задачі та забороняємо запуск інших сервісів на ній

```sh
kubectl label node <node-name> heavyload=true
kubectl taint nodes <node-name> heavyload=true:NoSchedule
```

Прописуємо nodeSelector та tolerations для нашого workloadengine поду, таким чином
дозволяючи йому і лише йому працювати на цій ноді

```yaml
    spec:
      nodeSelector:
        heavyload: "true"
      tolerations:
        - key: "heavyload"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
```

Так як цей сервіс потребує багато ресурсів і має виділену ноду - обмеження ресурсів можна не ставити.

- додаток, моніторинг та інше

Дані задачі не вимагають багато ресурсів та будуть розподілені автоматично кубером в залежності від запитів.
Необхідно провести профілювання не лише завантаження процесора, а й використання оперативної пам'яті да диску.
Після цього задаються наступні значення (приклад з доки):


```yaml
          resources:
            requests:
              cpu: "500m"      # requests half a CPU core
              memory: "256Mi" # requests 256 MB RAM
            limits:
              cpu: "1"        # max 1 full CPU core
              memory: "512Mi" # max 512 MB RAM
```

kube-scheduler автоматично розмістить поди на різні ноди відповідно до їх потреб, намагаючись зробити це рівномірно.
Але за потреби можна розподілити все вручну, як і попередні поди.

Не має значення, де розгорнуті поди, так як віртуальна мережа кластера дозволяє сервісам спілкуватись
не звертаючи уваги на те, де вони фактично працюють.

## Варіант 2

- база (redis) та додаток на виділеній ноді (low network latency)
- workloadengine на виділеній ноді
- окрема нода для моніторингу та іншого

## Варіант 3

- workloadengine на виділеній ноді разом з додатком, якщо вони багато взаємодіють, там же можливо редіс
- бази та інші сервіси розподілені по іншим нодам (1 чи 2)

## Варіант 4 (логічний)

- база (redis, postgres) на своїй ноді
- моніторинг-база (quest, grafana, seq) на своїй ноді, тоді  quest і postgres не конфліктують за disk i/o
- Workload Engine + .NET + Blazor + NATS

---

# Summary

Кубер абсолютно не обмежує нас у виборі варіантів та дозволяє гнучко налаштовувати систему навіть прямо на проді.
Можна додати автоскейлинг, можна додавати ноди за потреби, можна переїхати на ECS чи GCP чи на іншого VPS провайдера.
